{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scipy.io as scio\n",
    "import random\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前样本数量:35351\n",
      "标签 1 的个数：1551\n",
      "标签 2 的个数：1156\n",
      "标签 3 的个数：997\n",
      "标签 4 的个数：877\n",
      "标签 5 的个数：777\n",
      "标签 6 的个数：489\n",
      "标签 7 的个数：1138\n",
      "标签 8 的个数：542\n",
      "标签 9 的个数：774\n",
      "标签 10 的个数：878\n",
      "标签 11 的个数：846\n",
      "标签 12 的个数：858\n",
      "标签 13 的个数：762\n",
      "标签 14 的个数：718\n",
      "标签 15 的个数：448\n",
      "标签 16 的个数：861\n",
      "标签 17 的个数：794\n",
      "处理后样本数量:14466\n",
      "训练集大小为:8679\n",
      "测试集大小为:5787\n"
     ]
    }
   ],
   "source": [
    "def LoadMat(path):\n",
    "    mat = scio.loadmat(path)\n",
    "    emg = mat['emg']\n",
    "    label = []\n",
    "    try:\n",
    "        label = mat['restimulus']\n",
    "    except:\n",
    "        label = mat['stimulus']\n",
    "    return emg, label\n",
    "\n",
    "\n",
    "def cut(emg, label):\n",
    "    ans_emg = []\n",
    "    ans_label = []\n",
    "    i = 0\n",
    "    while(i < len(label)):\n",
    "        if(i + 199 >= len(label) - 1):\n",
    "            break\n",
    "        if(label[i] == label[i+199]):\n",
    "            start = i\n",
    "            end = i + 200\n",
    "            temp_emg = emg[start : end]\n",
    "            temp_label = label[i]\n",
    "            ans_emg.append(temp_emg)\n",
    "            ans_label.append(temp_label)\n",
    "            i += 50\n",
    "        else:\n",
    "            i += 50\n",
    "    return ans_emg, ans_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_unique_labels(labels):\n",
    "    label_count = {}  # 用字典来存储不同标签及其对应的个数\n",
    "\n",
    "    for label_list in labels:\n",
    "        label = label_list[0]  # 提取单个元素的标签\n",
    "        if label in label_count:\n",
    "            label_count[label] += 1\n",
    "        else:\n",
    "            label_count[label] = 1\n",
    "\n",
    "    for label, count in label_count.items():\n",
    "        print(f\"标签 {label} 的个数：{count}\")\n",
    "\n",
    "def one_hot(x, class_count=41):\n",
    "    return torch.eye(class_count)[x, :].squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 读取矩阵，并存储emg和label\n",
    "E1_emg, E1_label = LoadMat(\"D:/research\\EMG project\\DB2_s1\\S1_E1_A1.mat\")\n",
    "\n",
    "\n",
    "\n",
    "# 将同一志愿者的所有肌电信号数据进行切割后合并\n",
    "emgs, labels = cut(E1_emg, E1_label)\n",
    "# E2_emg, E2_label = cut(E2_emg, E2_label)\n",
    "# E3_emg, E3_label = cut(E3_emg, E3_label)\n",
    "# emgs.extend(E2_emg)\n",
    "# emgs.extend(E3_emg)\n",
    "# labels.extend(E2_label)\n",
    "# labels.extend(E3_label)\n",
    "\n",
    "# 查看数据集大小\n",
    "print('处理前样本数量:'+str(len(emgs)))  # 肌电信号数据集长度为102430\n",
    "\n",
    "# 发现标签为0的样本数量过多，剔除一部分\n",
    "# 找到所有标签为 0 的样本的索引\n",
    "label_0_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "\n",
    "# 随机选择 100 个标签为 0 的样本的索引\n",
    "selected_indices = random.sample(label_0_indices, min(53600, len(label_0_indices)))\n",
    "\n",
    "# 保留不在选定索引中的样本和标签\n",
    "emgs = [emgs[i] for i in range(len(emgs)) if i not in selected_indices]\n",
    "labels = [labels[i] for i in range(len(labels)) if i not in selected_indices]\n",
    "\n",
    "count_unique_labels(labels)  # 统计不同标签的数据样本量\n",
    "print('处理后样本数量:'+str(len(emgs)))\n",
    "\n",
    "# 切割训练集和测试集(6:4)\n",
    "train_emgs, test_emgs, train_labels, test_labels = train_test_split(emgs, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "print('训练集大小为:'+str(len(train_labels)))\n",
    "print('测试集大小为:'+str(len(test_labels)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本的形状为:torch.Size([8679, 200, 12])\n",
      "测试集样本的形状为:torch.Size([5787, 200, 12])\n",
      "训练集标签的形状为:torch.Size([8679, 1])\n",
      "测试集标签的形状为:torch.Size([5787, 1])\n",
      "样本的数据类型为:torch.float32\n",
      "标签的数据类型为:torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 将数据集转换为tensor\n",
    "#* 列表转array 转tensor\n",
    "train_emgs = np.array(train_emgs)\n",
    "train_labels = np.array(train_labels)\n",
    "test_emgs = np.array(test_emgs)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "train_emgs = torch.tensor(train_emgs)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.int64)   \n",
    "test_emgs = torch.tensor(test_emgs)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.int64)\n",
    "\n",
    "\n",
    "# 查看训练集和测试集的形状和数据类型\n",
    "print('训练集样本的形状为:'+str(train_emgs.shape))      #* 总共有多少段，每段长200，12个通道\n",
    "print('测试集样本的形状为:'+str(test_emgs.shape))\n",
    "print('训练集标签的形状为:'+str(train_labels.shape))\n",
    "print('测试集标签的形状为:'+str(test_labels.shape))\n",
    "\n",
    "print('样本的数据类型为:'+str(train_emgs.dtype))  # emg信号的数据类型为float32\n",
    "print('标签的数据类型为:'+str(train_labels.dtype))  # label的数据类型为int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个批次的样本的形状:torch.Size([32, 200, 12])\n",
      "一个批次的标签的形状:torch.Size([32, 41])\n"
     ]
    }
   ],
   "source": [
    "class EMG_dataset(data.Dataset):\n",
    "    def __init__(self, emgs, labels):\n",
    "        self.emgs = emgs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        emg = emgs[index]\n",
    "        label = labels[index]\n",
    "        label = one_hot(label)     #* 标签为onehot\n",
    "        return emg, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emgs)\n",
    "\n",
    "\n",
    "# 创建训练集的 EMG_dataset 和 DataLoader\n",
    "train_dataset = EMG_dataset(train_emgs, train_labels)\n",
    "\n",
    "#todo :\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)      #!这里处理一个批次32\n",
    "\n",
    "# 创建测试集的 EMG_dataset 和 DataLoader\n",
    "test_dataset = EMG_dataset(test_emgs, test_labels)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# 取出一个批次的数据查看其形状\n",
    "emg, label = next(iter(train_dataloader))\n",
    "print('一个批次的样本的形状:'+str(emg.shape))  # batch, 200, 12\n",
    "print('一个批次的标签的形状:'+str(label.shape))  # batch, 1    #* 标签长度为41维onehot向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成器\n",
    "\n",
    "encoder\n",
    "decoder\n",
    "- 调用attention 解码encoder的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention part\n",
    "\n",
    "- more to add:\n",
    "- location function --  calculate location info in seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random  # 你可能需要导入 random 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scipy.io as scio\n",
    "import random\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init   #* 根据语音生成模型，选用xavier初始化\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, hidden_dim) :\n",
    "\t\tsuper(Attention,self).__init__()\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\t\n",
    "\t\tself.attention = nn.Linear(hidden_dim * 2, hidden_dim)    #*连接编码器和解码器的隐藏状态后，调整维度\n",
    "\t\tself. value = nn.Parameter(init.xavier_normal_(torch.empty(hidden_dim)))\n",
    "\n",
    "    #todo : 前向传播\n",
    "\tdef forward(self, hidden, encoder_outputs):\n",
    "\t\tseq_length = encoder_outputs.shape[0]   #* 编码器输出序列长度为序列长度\n",
    "\t\tattention_energy = torch.zeros(seq_length)   #*初始化注意力能量--0\n",
    "\n",
    "\t\tfor i in range(seq_length):\n",
    "\t\t\tattention_energy[i] = self.score(hidden, encoder_outputs[i])\n",
    "\t\t\n",
    "\t\tattention_weight = torch.softmax(attention_energy, dim=0)   \n",
    "\t\t#* 计算权重\n",
    "\n",
    "\t\tcontext = context = torch.sum(attention_weight * encoder_outputs, dim=0)\n",
    "        \n",
    "\t\treturn context, attention_weight\n",
    "\t\n",
    "\n",
    "\t#todo: score\n",
    "\tdef score(self, hidden , encoder_output) :\n",
    "\n",
    "\t\t#* 需要 hidden隐藏层和 编码器输出为维度相同的向量 -- 相乘\n",
    "\t\tenergy = torch.dot(hidden, encoder_output)\n",
    "\t\treturn energy\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, input_dim, hidden_dim, num_layers,dropout):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "\t\tself.rnn = nn.LSTM( hidden_dim, num_layers,dropout = dropout)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t# todo : to imporve !!!\n",
    "\t#* directly use RNN in forward\n",
    "\tdef forward (self,src):\n",
    "\t\toutputs, (hidden, cell) = self.rnn(src)\n",
    "\t\treturn hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, output_dim, emb_dim, hidden_dim, num_layers, dropout) :\n",
    "\t\tsuper(Decoder,self).__init__()\n",
    "\t\tself.output_dim = output_dim\n",
    "\t\tself.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\t\tself.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\t\n",
    "\t\t#* out = in * W(t) + bias\n",
    "\t\tself.function_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\tdef forward(self, input, hidden, cell):\n",
    "\t\tinput = input.unsqueeze(0)\n",
    "\t\tembedded = self.dropout(self.embedding(input))\n",
    "\t\toutput, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\t\tprediction = self.function_output(output.squeeze(0))\n",
    "\t\treturn prediction, hidden, cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module) :\n",
    "\tdef __init__(self, encoder,decoder) :\n",
    "\t\tsuper(Generator,self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\t\t\n",
    "\t\n",
    "\t\n",
    "\t#* input tensor的维度(seq 长度，batch，输入特征维度)\n",
    "\tdef forward(self,input_tensor,target , teacher_forcing_ratio=0.5):\n",
    "\t\t#* initialize  bianliang\n",
    "\t\ttarget_length, batch_size ,  = target.shape\n",
    "\t\ttarget_ini_size = self.decoder.output_dim\n",
    "\n",
    "\t\toutputs = torch.zeros(target_length, batch_size, target_ini_size).to(self.device)\n",
    "\t\thidden, cell = self.encoder(input_tensor)\n",
    "\t\tinput = input_tensor[0, :, : ].unsqueeze(0)    #* adjust input dim\n",
    "\n",
    "\n",
    "\t\tfor time in range (1, target_length) :\n",
    "\t\t\toutput ,hidden,cell = self.decoder(input, hidden, cell)\n",
    "\t\t\toutputs[time] = output\n",
    "\t\t\tteacher_force = random.random() < teacher_forcing_ratio\n",
    "\t\t\ttop1 = output.argmax(2) \n",
    "\t\t\tinput = target[time:time+1, :, :] if teacher_force else top1\n",
    "\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to be continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\PYTHON 上课\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m trg \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, output_dim, (seq_length, batch_size, output_dim))  \u001b[39m# 目标序列\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# 模型前向传播\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m output \u001b[39m=\u001b[39m model(src, trg)\n",
      "File \u001b[1;32mf:\\PYTHON 上课\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, input_tensor, target, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,input_tensor,target , teacher_forcing_ratio\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[0;32m     11\u001b[0m \t\u001b[39m#* initialize  bianliang\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \ttarget_length, batch_size ,  \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mshape\n\u001b[0;32m     13\u001b[0m \ttarget_ini_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39moutput_dim\n\u001b[0;32m     15\u001b[0m \toutputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(target_length, batch_size, target_ini_size)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
